import os
import time
import requests
import urllib3
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By

from crawler.ComicCrawler import ComicCrawler

# B·ªè c·∫£nh b√°o SSL n·∫øu d√πng verify=False
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class UndetectedChromeCrawler(ComicCrawler):
    def __init__(self):
        super().__init__()
        self.comic_name = 'invincible'  # Default comic name

        # ===== C·∫§U H√åNH TR√åNH DUY·ªÜT (·∫©n bot) =====
        options = uc.ChromeOptions()
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--start-maximized")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36")

        # ===== KH·ªûI T·∫†O TR√åNH DUY·ªÜT CH·ªêNG CLOUDLARE =====
        self.driver = uc.Chrome(options=options)
        self.driver.implicitly_wait(10)

    def download_images_from_chapter(self, chapter_url, save_root=None):
        if save_root is None:
            save_root = self.comic_name

        print(f"üîó Visiting: {chapter_url}")
        self.driver.get(chapter_url)
        time.sleep(6)  # ƒê·ª£i JS + Cloudflare

        if "Cloudflare" in self.driver.title or "blocked" in self.driver.page_source:
            print("‚ùå B·ªã Cloudflare ch·∫∑n.")
            return None

        # T·∫°o th∆∞ m·ª•c l∆∞u ·∫£nh
        chapter_name = chapter_url.strip("/").split("/")[-1]
        save_folder = os.path.join(save_root, chapter_name)
        os.makedirs(save_folder, exist_ok=True)

        # T√¨m ·∫£nh trong div.list-images
        image_elements = self.driver.find_elements(By.CSS_SELECTOR, "div.list-images img")
        print(f"üì∏ Found {len(image_elements)} images")

        for i, img in enumerate(image_elements):
            img_url = img.get_attribute("src") or img.get_attribute("data-src") or img.get_attribute("alt")
            if not img_url or not img_url.startswith("http"):
                print(f"‚ö†Ô∏è Skipped image at index {i}")
                continue

            try:
                img_data = requests.get(img_url, verify=False).content
                file_path = os.path.join(save_folder, f"{i+1:02}.jpg")
                with open(file_path, "wb") as f:
                    f.write(img_data)
                print(f"‚úÖ Saved: {file_path}")
            except Exception as e:
                print(f"‚ùå Failed to download image: {e}")

        print(f"‚úÖ Completed chapter: {chapter_name}")

        # T√¨m ch∆∞∆°ng k·∫ø ti·∫øp: n√∫t "Issue sau"
        try:
            next_button = self.driver.find_element(
                By.XPATH, '//a[@class="button primary is-small"][span[contains(text(),"Issue sau")]]'
            )
            return next_button.get_attribute("href")
        except:
            print("‚õî Kh√¥ng c√≤n ch∆∞∆°ng ti·∫øp theo.")
            return None

    def crawl(self, url, comic_name=None, chapter_name=None):
        if comic_name:
            self.comic_name = comic_name

        current_url = url

        while current_url:
            current_url = self.download_images_from_chapter(current_url)
            time.sleep(2)

        print("üéâ Xong h·∫øt r·ªìi!")

    def web_code(self):
        return "LANGGEEK_UNDETECTED"


# Compatibility function for crawler_manager
def crawler(url, comic_name='invincible', chapter_name=None):
    """
    Compatibility function for the crawler_manager.

    Args:
        url (str): URL to crawl
        comic_name (str, optional): Name of the comic. Defaults to 'invincible'.
        chapter_name (str, optional): Not used in this crawler but kept for compatibility.
    """
    crawler_instance = UndetectedChromeCrawler()
    crawler_instance.crawl(url, comic_name)
    crawler_instance.close()


# For direct execution
if __name__ == "__main__":
    start_url = "https://langgeek.net/invincible/chuong-1-50/"  # ‚úÖ THAY ƒê·ªîI n·∫øu c·∫ßn
    comic_name = 'invincible'
    crawler(start_url, comic_name)
